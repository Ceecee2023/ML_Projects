{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "## Peihao Chen / Siqi Wang\n",
    "### 2023-12-29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Load data and understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(418, 11)\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_data.head())\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Check the data type and missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "PassengerId    0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check discrete and continuous variables\n",
    "print(train_data.info())\n",
    "# Check missing values\n",
    "print(train_data.isnull().sum())\n",
    "# too many missing values in Cabin, so drop it\n",
    "train_data.drop('Cabin', axis=1, inplace=True)\n",
    "test_data.drop('Cabin', axis=1, inplace=True)\n",
    "# drop other Nan values\n",
    "# train_data.dropna(inplace=True)\n",
    "# test_data.dropna(inplace=True)\n",
    "# use SimpleImputer to fill missing values(age, fare, Embarked)\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit(train_data[['Embarked']])\n",
    "train_data['Embarked'] = imputer.transform(train_data[['Embarked']]).ravel()\n",
    "test_data['Embarked'] = imputer.transform(test_data[['Embarked']]).ravel()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(train_data[['Age']])\n",
    "train_data['Age'] = imputer.transform(train_data[['Age']]).ravel()\n",
    "test_data['Age'] = imputer.transform(test_data[['Age']]).ravel()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(train_data[['Fare']])\n",
    "train_data['Fare'] = imputer.transform(train_data[['Fare']]).ravel()\n",
    "test_data['Fare'] = imputer.transform(test_data[['Fare']]).ravel()\n",
    "# check missing values again\n",
    "print(train_data.isnull().sum())\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "0         0       3    male  22.0      1      0   7.2500        S\n",
      "1         1       1  female  38.0      1      0  71.2833        C\n",
      "2         1       3  female  26.0      0      0   7.9250        S\n",
      "3         1       1  female  35.0      1      0  53.1000        S\n",
      "4         0       3    male  35.0      0      0   8.0500        S\n"
     ]
    }
   ],
   "source": [
    "# drop unnecessary columns\n",
    "train_data.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "test_data.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass   Age  SibSp  Parch     Fare  Sex_female  Sex_male  \\\n",
      "0         0       3  22.0      1      0   7.2500       False      True   \n",
      "1         1       1  38.0      1      0  71.2833        True     False   \n",
      "2         1       3  26.0      0      0   7.9250        True     False   \n",
      "3         1       1  35.0      1      0  53.1000        True     False   \n",
      "4         0       3  35.0      0      0   8.0500       False      True   \n",
      "\n",
      "   Embarked_C  Embarked_Q  Embarked_S  \n",
      "0       False       False        True  \n",
      "1        True       False       False  \n",
      "2       False       False        True  \n",
      "3       False       False        True  \n",
      "4       False       False        True  \n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "train_data = pd.get_dummies(train_data)\n",
    "test_data = pd.get_dummies(test_data)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split data and standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = train_data.drop('Survived', axis=1)\n",
    "y_train  = train_data['Survived']\n",
    "# Standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 1, Loss: 0.6817579865455627\n",
      "Epoch 2, Loss: 0.546280562877655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6164097785949707\n",
      "Epoch 4, Loss: 0.3713301122188568\n",
      "Epoch 5, Loss: 0.4536885619163513\n",
      "Epoch 6, Loss: 0.5752047896385193\n",
      "Epoch 7, Loss: 0.3106324374675751\n",
      "Epoch 8, Loss: 0.49496254324913025\n",
      "Epoch 9, Loss: 0.4464777708053589\n",
      "Epoch 10, Loss: 0.5585728883743286\n",
      "Epoch 11, Loss: 0.4213184714317322\n",
      "Epoch 12, Loss: 0.5134940147399902\n",
      "Epoch 13, Loss: 0.3243294358253479\n",
      "Epoch 14, Loss: 0.42586416006088257\n",
      "Epoch 15, Loss: 0.574256420135498\n",
      "Epoch 16, Loss: 0.5606386661529541\n",
      "Epoch 17, Loss: 0.7193633913993835\n",
      "Epoch 18, Loss: 0.4240049421787262\n",
      "Epoch 19, Loss: 0.3776012361049652\n",
      "Epoch 20, Loss: 0.47150593996047974\n",
      "Epoch 21, Loss: 0.326913058757782\n",
      "Epoch 22, Loss: 0.30144190788269043\n",
      "Epoch 23, Loss: 0.4492599368095398\n",
      "Epoch 24, Loss: 0.5764710903167725\n",
      "Epoch 25, Loss: 0.31903108954429626\n",
      "Epoch 26, Loss: 0.4544661343097687\n",
      "Epoch 27, Loss: 0.4396928548812866\n",
      "Epoch 28, Loss: 0.40625011920928955\n",
      "Epoch 29, Loss: 0.44414448738098145\n",
      "Epoch 30, Loss: 0.20611783862113953\n",
      "Epoch 31, Loss: 0.38644880056381226\n",
      "Epoch 32, Loss: 0.44391992688179016\n",
      "Epoch 33, Loss: 0.36097171902656555\n",
      "Epoch 34, Loss: 0.5515370965003967\n",
      "Epoch 35, Loss: 0.29901841282844543\n",
      "Epoch 36, Loss: 0.2286330908536911\n",
      "Epoch 37, Loss: 0.5111056566238403\n",
      "Epoch 38, Loss: 0.5685248374938965\n",
      "Epoch 39, Loss: 0.3944856524467468\n",
      "Epoch 40, Loss: 0.5155348777770996\n",
      "Epoch 41, Loss: 0.49288973212242126\n",
      "Epoch 42, Loss: 0.5492649078369141\n",
      "Epoch 43, Loss: 0.4213888347148895\n",
      "Epoch 44, Loss: 0.49146801233291626\n",
      "Epoch 45, Loss: 0.40342190861701965\n",
      "Epoch 46, Loss: 0.4830781817436218\n",
      "Epoch 47, Loss: 0.3934377431869507\n",
      "Epoch 48, Loss: 0.38945329189300537\n",
      "Epoch 49, Loss: 0.39387086033821106\n",
      "Epoch 50, Loss: 0.3703784942626953\n",
      "Epoch 51, Loss: 0.41485264897346497\n",
      "Epoch 52, Loss: 0.25314706563949585\n",
      "Epoch 53, Loss: 0.4828571677207947\n",
      "Epoch 54, Loss: 0.3946966826915741\n",
      "Epoch 55, Loss: 0.48107269406318665\n",
      "Epoch 56, Loss: 0.34517550468444824\n",
      "Epoch 57, Loss: 0.3401331305503845\n",
      "Epoch 58, Loss: 0.49226295948028564\n",
      "Epoch 59, Loss: 0.20441663265228271\n",
      "Epoch 60, Loss: 0.3917337656021118\n",
      "Epoch 61, Loss: 0.3688357174396515\n",
      "Epoch 62, Loss: 0.269834041595459\n",
      "Epoch 63, Loss: 0.23245570063591003\n",
      "Epoch 64, Loss: 0.45350608229637146\n",
      "Epoch 65, Loss: 0.31346365809440613\n",
      "Epoch 66, Loss: 0.46194779872894287\n",
      "Epoch 67, Loss: 0.24926058948040009\n",
      "Epoch 68, Loss: 0.6383715867996216\n",
      "Epoch 69, Loss: 0.2521527111530304\n",
      "Epoch 70, Loss: 0.37425360083580017\n",
      "Epoch 71, Loss: 0.6043036580085754\n",
      "Epoch 72, Loss: 0.20258201658725739\n",
      "Epoch 73, Loss: 0.7022308707237244\n",
      "Epoch 74, Loss: 0.33046290278434753\n",
      "Epoch 75, Loss: 0.26953819394111633\n",
      "Epoch 76, Loss: 0.5099401473999023\n",
      "Epoch 77, Loss: 0.4391443133354187\n",
      "Epoch 78, Loss: 0.5766509175300598\n",
      "Epoch 79, Loss: 0.47140419483184814\n",
      "Epoch 80, Loss: 0.31978657841682434\n",
      "Epoch 81, Loss: 0.5339582562446594\n",
      "Epoch 82, Loss: 0.2667565643787384\n",
      "Epoch 83, Loss: 0.5037855505943298\n",
      "Epoch 84, Loss: 0.4874817430973053\n",
      "Epoch 85, Loss: 0.49450892210006714\n",
      "Epoch 86, Loss: 0.26912862062454224\n",
      "Epoch 87, Loss: 0.428415983915329\n",
      "Epoch 88, Loss: 0.512514591217041\n",
      "Epoch 89, Loss: 0.44534003734588623\n",
      "Epoch 90, Loss: 0.3536474108695984\n",
      "Epoch 91, Loss: 0.41488054394721985\n",
      "Epoch 92, Loss: 0.2939811944961548\n",
      "Epoch 93, Loss: 0.30410975217819214\n",
      "Epoch 94, Loss: 0.42674753069877625\n",
      "Epoch 95, Loss: 0.23257973790168762\n",
      "Epoch 96, Loss: 0.2960040271282196\n",
      "Epoch 97, Loss: 0.45278099179267883\n",
      "Epoch 98, Loss: 0.7097697854042053\n",
      "Epoch 99, Loss: 0.29621219635009766\n",
      "Epoch 100, Loss: 0.3888356387615204\n",
      "Epoch 101, Loss: 0.36499208211898804\n",
      "Epoch 102, Loss: 0.4152372479438782\n",
      "Epoch 103, Loss: 0.5299320816993713\n",
      "Epoch 104, Loss: 0.22593599557876587\n",
      "Epoch 105, Loss: 0.5745171308517456\n",
      "Epoch 106, Loss: 0.33635324239730835\n",
      "Epoch 107, Loss: 0.4045078754425049\n",
      "Epoch 108, Loss: 0.3442631959915161\n",
      "Epoch 109, Loss: 0.4961763918399811\n",
      "Epoch 110, Loss: 0.6054118871688843\n",
      "Epoch 111, Loss: 0.27537503838539124\n",
      "Epoch 112, Loss: 0.4062559902667999\n",
      "Epoch 113, Loss: 0.37451857328414917\n",
      "Epoch 114, Loss: 0.630280077457428\n",
      "Epoch 115, Loss: 0.42272183299064636\n",
      "Epoch 116, Loss: 0.45220550894737244\n",
      "Epoch 117, Loss: 0.4260543882846832\n",
      "Epoch 118, Loss: 0.3870582580566406\n",
      "Epoch 119, Loss: 0.39248862862586975\n",
      "Epoch 120, Loss: 0.3509383499622345\n",
      "Epoch 121, Loss: 0.24495349824428558\n",
      "Epoch 122, Loss: 0.33435744047164917\n",
      "Epoch 123, Loss: 0.427896112203598\n",
      "Epoch 124, Loss: 0.4145161807537079\n",
      "Epoch 125, Loss: 0.3449068069458008\n",
      "Epoch 126, Loss: 0.36821573972702026\n",
      "Epoch 127, Loss: 0.4221523106098175\n",
      "Epoch 128, Loss: 0.29724910855293274\n",
      "Epoch 129, Loss: 0.3152609169483185\n",
      "Epoch 130, Loss: 0.3024403750896454\n",
      "Epoch 131, Loss: 0.2834310829639435\n",
      "Epoch 132, Loss: 0.4490301311016083\n",
      "Epoch 133, Loss: 0.44216033816337585\n",
      "Epoch 134, Loss: 0.3151824474334717\n",
      "Epoch 135, Loss: 0.30022501945495605\n",
      "Epoch 136, Loss: 0.45676639676094055\n",
      "Epoch 137, Loss: 0.40287476778030396\n",
      "Epoch 138, Loss: 0.4877493679523468\n",
      "Epoch 139, Loss: 0.47698974609375\n",
      "Epoch 140, Loss: 0.39839091897010803\n",
      "Epoch 141, Loss: 0.4227319061756134\n",
      "Epoch 142, Loss: 0.4351555407047272\n",
      "Epoch 143, Loss: 0.2871696949005127\n",
      "Epoch 144, Loss: 0.2837260067462921\n",
      "Epoch 145, Loss: 0.3660804033279419\n",
      "Epoch 146, Loss: 0.2997266352176666\n",
      "Epoch 147, Loss: 0.38043177127838135\n",
      "Epoch 148, Loss: 0.3465861976146698\n",
      "Epoch 149, Loss: 0.4198411703109741\n",
      "Epoch 150, Loss: 0.26411116123199463\n",
      "Epoch 151, Loss: 0.4566890001296997\n",
      "Epoch 152, Loss: 0.2897377610206604\n",
      "Epoch 153, Loss: 0.343955934047699\n",
      "Epoch 154, Loss: 0.325135737657547\n",
      "Epoch 155, Loss: 0.5731393098831177\n",
      "Epoch 156, Loss: 0.502319872379303\n",
      "Epoch 157, Loss: 0.3583213686943054\n",
      "Epoch 158, Loss: 0.29059404134750366\n",
      "Epoch 159, Loss: 0.4057893753051758\n",
      "Epoch 160, Loss: 0.277482807636261\n",
      "Epoch 161, Loss: 0.2656957805156708\n",
      "Epoch 162, Loss: 0.5933549404144287\n",
      "Epoch 163, Loss: 0.5582423210144043\n",
      "Epoch 164, Loss: 0.2953133285045624\n",
      "Epoch 165, Loss: 0.35695716738700867\n",
      "Epoch 166, Loss: 0.37938192486763\n",
      "Epoch 167, Loss: 0.3834421634674072\n",
      "Epoch 168, Loss: 0.35132545232772827\n",
      "Epoch 169, Loss: 0.38287660479545593\n",
      "Epoch 170, Loss: 0.39373207092285156\n",
      "Epoch 171, Loss: 0.5368613004684448\n",
      "Epoch 172, Loss: 0.20295020937919617\n",
      "Epoch 173, Loss: 0.46493181586265564\n",
      "Epoch 174, Loss: 0.46476808190345764\n",
      "Epoch 175, Loss: 0.6186928153038025\n",
      "Epoch 176, Loss: 0.3092298209667206\n",
      "Epoch 177, Loss: 0.39462029933929443\n",
      "Epoch 178, Loss: 0.6150992512702942\n",
      "Epoch 179, Loss: 0.27650368213653564\n",
      "Epoch 180, Loss: 0.33179396390914917\n",
      "Epoch 181, Loss: 0.4534488320350647\n",
      "Epoch 182, Loss: 0.40245798230171204\n",
      "Epoch 183, Loss: 0.4337975084781647\n",
      "Epoch 184, Loss: 0.29676756262779236\n",
      "Epoch 185, Loss: 0.5113741755485535\n",
      "Epoch 186, Loss: 0.42804667353630066\n",
      "Epoch 187, Loss: 0.722042441368103\n",
      "Epoch 188, Loss: 0.267454594373703\n",
      "Epoch 189, Loss: 0.22640909254550934\n",
      "Epoch 190, Loss: 0.4587665796279907\n",
      "Epoch 191, Loss: 0.17480605840682983\n",
      "Epoch 192, Loss: 0.3699335753917694\n",
      "Epoch 193, Loss: 0.34367042779922485\n",
      "Epoch 194, Loss: 0.42883047461509705\n",
      "Epoch 195, Loss: 0.3978789746761322\n",
      "Epoch 196, Loss: 0.5663547515869141\n",
      "Epoch 197, Loss: 0.38397595286369324\n",
      "Epoch 198, Loss: 0.29790347814559937\n",
      "Epoch 199, Loss: 0.6337909698486328\n",
      "Epoch 200, Loss: 0.22096329927444458\n",
      "Epoch 201, Loss: 0.5974254012107849\n",
      "Epoch 202, Loss: 0.5419593453407288\n",
      "Epoch 203, Loss: 0.37879106402397156\n",
      "Epoch 204, Loss: 0.4420037865638733\n",
      "Epoch 205, Loss: 0.26900696754455566\n",
      "Epoch 206, Loss: 0.2526128590106964\n",
      "Epoch 207, Loss: 0.17788197100162506\n",
      "Epoch 208, Loss: 0.43530580401420593\n",
      "Epoch 209, Loss: 0.3335724174976349\n",
      "Epoch 210, Loss: 0.5357001423835754\n",
      "Epoch 211, Loss: 0.23423193395137787\n",
      "Epoch 212, Loss: 0.2661532461643219\n",
      "Epoch 213, Loss: 0.3596963882446289\n",
      "Epoch 214, Loss: 0.4589841663837433\n",
      "Epoch 215, Loss: 0.4207327365875244\n",
      "Epoch 216, Loss: 0.32669833302497864\n",
      "Epoch 217, Loss: 0.44637298583984375\n",
      "Epoch 218, Loss: 0.36608758568763733\n",
      "Epoch 219, Loss: 0.3608577847480774\n",
      "Epoch 220, Loss: 0.5734155178070068\n",
      "Epoch 221, Loss: 0.19714011251926422\n",
      "Epoch 222, Loss: 0.4378717839717865\n",
      "Epoch 223, Loss: 0.2951747179031372\n",
      "Epoch 224, Loss: 0.17019103467464447\n",
      "Epoch 225, Loss: 0.31334319710731506\n",
      "Epoch 226, Loss: 0.23984210193157196\n",
      "Epoch 227, Loss: 0.33026719093322754\n",
      "Epoch 228, Loss: 0.515821635723114\n",
      "Epoch 229, Loss: 0.33715689182281494\n",
      "Epoch 230, Loss: 0.2729182243347168\n",
      "Epoch 231, Loss: 0.319263219833374\n",
      "Epoch 232, Loss: 0.3487054407596588\n",
      "Epoch 233, Loss: 0.36787426471710205\n",
      "Epoch 234, Loss: 0.2980045676231384\n",
      "Epoch 235, Loss: 0.2242240607738495\n",
      "Epoch 236, Loss: 0.528517484664917\n",
      "Epoch 237, Loss: 0.4554072618484497\n",
      "Epoch 238, Loss: 0.4683588147163391\n",
      "Epoch 239, Loss: 0.3114253580570221\n",
      "Epoch 240, Loss: 0.2763339579105377\n",
      "Epoch 241, Loss: 0.5711598992347717\n",
      "Epoch 242, Loss: 0.3601853549480438\n",
      "Epoch 243, Loss: 0.3388114273548126\n",
      "Epoch 244, Loss: 0.5463124513626099\n",
      "Epoch 245, Loss: 0.37515586614608765\n",
      "Epoch 246, Loss: 0.42819246649742126\n",
      "Epoch 247, Loss: 0.219304621219635\n",
      "Epoch 248, Loss: 0.6512669324874878\n",
      "Epoch 249, Loss: 0.33326706290245056\n",
      "Epoch 250, Loss: 0.3595390319824219\n",
      "Epoch 251, Loss: 0.3110434114933014\n",
      "Epoch 252, Loss: 0.42795661091804504\n",
      "Epoch 253, Loss: 0.34138160943984985\n",
      "Epoch 254, Loss: 0.5095917582511902\n",
      "Epoch 255, Loss: 0.4118087887763977\n",
      "Epoch 256, Loss: 0.3168918490409851\n",
      "Epoch 257, Loss: 0.43261927366256714\n",
      "Epoch 258, Loss: 0.3948560357093811\n",
      "Epoch 259, Loss: 0.5085791945457458\n",
      "Epoch 260, Loss: 0.3544594347476959\n",
      "Epoch 261, Loss: 0.4456169009208679\n",
      "Epoch 262, Loss: 0.42351609468460083\n",
      "Epoch 263, Loss: 0.3894345760345459\n",
      "Epoch 264, Loss: 0.25359511375427246\n",
      "Epoch 265, Loss: 0.24791304767131805\n",
      "Epoch 266, Loss: 0.3167332410812378\n",
      "Epoch 267, Loss: 0.3509296774864197\n",
      "Epoch 268, Loss: 0.23182789981365204\n",
      "Epoch 269, Loss: 0.33140385150909424\n",
      "Epoch 270, Loss: 0.5315507054328918\n",
      "Epoch 271, Loss: 0.408920556306839\n",
      "Epoch 272, Loss: 0.2836792469024658\n",
      "Epoch 273, Loss: 0.6384987831115723\n",
      "Epoch 274, Loss: 0.5477213859558105\n",
      "Epoch 275, Loss: 0.2291230410337448\n",
      "Epoch 276, Loss: 0.25296056270599365\n",
      "Epoch 277, Loss: 0.4510711133480072\n",
      "Epoch 278, Loss: 0.314167857170105\n",
      "Epoch 279, Loss: 0.29365280270576477\n",
      "Epoch 280, Loss: 0.3589228391647339\n",
      "Epoch 281, Loss: 0.3761652410030365\n",
      "Epoch 282, Loss: 0.4658477008342743\n",
      "Epoch 283, Loss: 0.3144288659095764\n",
      "Epoch 284, Loss: 0.32158157229423523\n",
      "Epoch 285, Loss: 0.3338073492050171\n",
      "Epoch 286, Loss: 0.244713693857193\n",
      "Epoch 287, Loss: 0.47303274273872375\n",
      "Epoch 288, Loss: 0.40037062764167786\n",
      "Epoch 289, Loss: 0.6217061281204224\n",
      "Epoch 290, Loss: 0.394363135099411\n",
      "Epoch 291, Loss: 0.24489469826221466\n",
      "Epoch 292, Loss: 0.25772613286972046\n",
      "Epoch 293, Loss: 0.4879809021949768\n",
      "Epoch 294, Loss: 0.35614338517189026\n",
      "Epoch 295, Loss: 0.37588655948638916\n",
      "Epoch 296, Loss: 0.39127659797668457\n",
      "Epoch 297, Loss: 0.3692042827606201\n",
      "Epoch 298, Loss: 0.12882840633392334\n",
      "Epoch 299, Loss: 0.44054970145225525\n",
      "Epoch 300, Loss: 0.31161683797836304\n",
      "Epoch 301, Loss: 0.380746990442276\n",
      "Epoch 302, Loss: 0.48677361011505127\n",
      "Epoch 303, Loss: 0.2037779539823532\n",
      "Epoch 304, Loss: 0.5063908100128174\n",
      "Epoch 305, Loss: 0.24409864842891693\n",
      "Epoch 306, Loss: 0.37517502903938293\n",
      "Epoch 307, Loss: 0.23482602834701538\n",
      "Epoch 308, Loss: 0.27661585807800293\n",
      "Epoch 309, Loss: 0.37029698491096497\n",
      "Epoch 310, Loss: 0.26397767663002014\n",
      "Epoch 311, Loss: 0.32712242007255554\n",
      "Epoch 312, Loss: 0.27087390422821045\n",
      "Epoch 313, Loss: 0.18465842306613922\n",
      "Epoch 314, Loss: 0.324844628572464\n",
      "Epoch 315, Loss: 0.4402335584163666\n",
      "Epoch 316, Loss: 0.1945454329252243\n",
      "Epoch 317, Loss: 0.2770210802555084\n",
      "Epoch 318, Loss: 0.407025545835495\n",
      "Epoch 319, Loss: 0.3322450518608093\n",
      "Epoch 320, Loss: 0.1946047991514206\n",
      "Epoch 321, Loss: 0.2696044147014618\n",
      "Epoch 322, Loss: 0.3350788652896881\n",
      "Epoch 323, Loss: 0.32070276141166687\n",
      "Epoch 324, Loss: 0.241726815700531\n",
      "Epoch 325, Loss: 0.365965336561203\n",
      "Epoch 326, Loss: 0.33377742767333984\n",
      "Epoch 327, Loss: 0.13993893563747406\n",
      "Epoch 328, Loss: 0.32405245304107666\n",
      "Epoch 329, Loss: 0.36891019344329834\n",
      "Epoch 330, Loss: 0.2904736399650574\n",
      "Epoch 331, Loss: 0.38584104180336\n",
      "Epoch 332, Loss: 0.5421893000602722\n",
      "Epoch 333, Loss: 0.47136399149894714\n",
      "Epoch 334, Loss: 0.3438292145729065\n",
      "Epoch 335, Loss: 0.4797521233558655\n",
      "Epoch 336, Loss: 0.32316461205482483\n",
      "Epoch 337, Loss: 0.2547014057636261\n",
      "Epoch 338, Loss: 0.24027682840824127\n",
      "Epoch 339, Loss: 0.252568781375885\n",
      "Epoch 340, Loss: 0.1995372623205185\n",
      "Epoch 341, Loss: 0.5798478126525879\n",
      "Epoch 342, Loss: 0.2558926045894623\n",
      "Epoch 343, Loss: 0.2890551686286926\n",
      "Epoch 344, Loss: 0.3484533727169037\n",
      "Epoch 345, Loss: 0.516525387763977\n",
      "Epoch 346, Loss: 0.5765115022659302\n",
      "Epoch 347, Loss: 0.2841673493385315\n",
      "Epoch 348, Loss: 0.41163119673728943\n",
      "Epoch 349, Loss: 0.21954956650733948\n",
      "Epoch 350, Loss: 0.3844646215438843\n",
      "Epoch 351, Loss: 0.48451197147369385\n",
      "Epoch 352, Loss: 0.30866891145706177\n",
      "Epoch 353, Loss: 0.2830042541027069\n",
      "Epoch 354, Loss: 0.21158026158809662\n",
      "Epoch 355, Loss: 0.25004246830940247\n",
      "Epoch 356, Loss: 0.43336713314056396\n",
      "Epoch 357, Loss: 0.2860490083694458\n",
      "Epoch 358, Loss: 0.45385509729385376\n",
      "Epoch 359, Loss: 0.20930826663970947\n",
      "Epoch 360, Loss: 0.6803240180015564\n",
      "Epoch 361, Loss: 0.45700058341026306\n",
      "Epoch 362, Loss: 0.31059837341308594\n",
      "Epoch 363, Loss: 0.252838671207428\n",
      "Epoch 364, Loss: 0.29764750599861145\n",
      "Epoch 365, Loss: 0.2942061722278595\n",
      "Epoch 366, Loss: 0.26285606622695923\n",
      "Epoch 367, Loss: 0.23053914308547974\n",
      "Epoch 368, Loss: 0.4427264928817749\n",
      "Epoch 369, Loss: 0.372005820274353\n",
      "Epoch 370, Loss: 0.35283637046813965\n",
      "Epoch 371, Loss: 0.42463117837905884\n",
      "Epoch 372, Loss: 0.16094791889190674\n",
      "Epoch 373, Loss: 0.36670953035354614\n",
      "Epoch 374, Loss: 0.2648988962173462\n",
      "Epoch 375, Loss: 0.29489028453826904\n",
      "Epoch 376, Loss: 0.4110259711742401\n",
      "Epoch 377, Loss: 0.2888856530189514\n",
      "Epoch 378, Loss: 0.2349063754081726\n",
      "Epoch 379, Loss: 0.20645488798618317\n",
      "Epoch 380, Loss: 0.3442596197128296\n",
      "Epoch 381, Loss: 0.40865975618362427\n",
      "Epoch 382, Loss: 0.4541723430156708\n",
      "Epoch 383, Loss: 0.32099267840385437\n",
      "Epoch 384, Loss: 0.2519236207008362\n",
      "Epoch 385, Loss: 0.2876655161380768\n",
      "Epoch 386, Loss: 0.2134302705526352\n",
      "Epoch 387, Loss: 0.21145015954971313\n",
      "Epoch 388, Loss: 0.33947446942329407\n",
      "Epoch 389, Loss: 0.32381904125213623\n",
      "Epoch 390, Loss: 0.4621903896331787\n",
      "Epoch 391, Loss: 0.31814971566200256\n",
      "Epoch 392, Loss: 0.2485324740409851\n",
      "Epoch 393, Loss: 0.24716588854789734\n",
      "Epoch 394, Loss: 0.23432254791259766\n",
      "Epoch 395, Loss: 0.29525625705718994\n",
      "Epoch 396, Loss: 0.2917475700378418\n",
      "Epoch 397, Loss: 0.3863096535205841\n",
      "Epoch 398, Loss: 0.2646620273590088\n",
      "Epoch 399, Loss: 0.37024304270744324\n",
      "Epoch 400, Loss: 0.32850050926208496\n",
      "Epoch 401, Loss: 0.24536210298538208\n",
      "Epoch 402, Loss: 0.22534941136837006\n",
      "Epoch 403, Loss: 0.35501235723495483\n",
      "Epoch 404, Loss: 0.32647377252578735\n",
      "Epoch 405, Loss: 0.17954035103321075\n",
      "Epoch 406, Loss: 0.3920244574546814\n",
      "Epoch 407, Loss: 0.3064386546611786\n",
      "Epoch 408, Loss: 0.23359088599681854\n",
      "Epoch 409, Loss: 0.25685596466064453\n",
      "Epoch 410, Loss: 0.18728774785995483\n",
      "Epoch 411, Loss: 0.29021957516670227\n",
      "Epoch 412, Loss: 0.3688148558139801\n",
      "Epoch 413, Loss: 0.2977183759212494\n",
      "Epoch 414, Loss: 0.44807934761047363\n",
      "Epoch 415, Loss: 0.32435569167137146\n",
      "Epoch 416, Loss: 0.37344399094581604\n",
      "Epoch 417, Loss: 0.38841432332992554\n",
      "Epoch 418, Loss: 0.15830451250076294\n",
      "Epoch 419, Loss: 0.3162732422351837\n",
      "Epoch 420, Loss: 0.31484848260879517\n",
      "Epoch 421, Loss: 0.36398786306381226\n",
      "Epoch 422, Loss: 0.3912988007068634\n",
      "Epoch 423, Loss: 0.2237807661294937\n",
      "Epoch 424, Loss: 0.2673977315425873\n",
      "Epoch 425, Loss: 0.258480429649353\n",
      "Epoch 426, Loss: 0.3952138125896454\n",
      "Epoch 427, Loss: 0.33946773409843445\n",
      "Epoch 428, Loss: 0.261783242225647\n",
      "Epoch 429, Loss: 0.3020531237125397\n",
      "Epoch 430, Loss: 0.20307934284210205\n",
      "Epoch 431, Loss: 0.297710657119751\n",
      "Epoch 432, Loss: 0.46384182572364807\n",
      "Epoch 433, Loss: 0.22496269643306732\n",
      "Epoch 434, Loss: 0.4356929361820221\n",
      "Epoch 435, Loss: 0.22799308598041534\n",
      "Epoch 436, Loss: 0.22417819499969482\n",
      "Epoch 437, Loss: 0.3337235152721405\n",
      "Epoch 438, Loss: 0.34509187936782837\n",
      "Epoch 439, Loss: 0.35709938406944275\n",
      "Epoch 440, Loss: 0.359231173992157\n",
      "Epoch 441, Loss: 0.27705106139183044\n",
      "Epoch 442, Loss: 0.34463539719581604\n",
      "Epoch 443, Loss: 0.31116142868995667\n",
      "Epoch 444, Loss: 0.23229263722896576\n",
      "Epoch 445, Loss: 0.23768818378448486\n",
      "Epoch 446, Loss: 0.27127406001091003\n",
      "Epoch 447, Loss: 0.5220046639442444\n",
      "Epoch 448, Loss: 0.4683702886104584\n",
      "Epoch 449, Loss: 0.4106121063232422\n",
      "Epoch 450, Loss: 0.3888222873210907\n",
      "Epoch 451, Loss: 0.4355998635292053\n",
      "Epoch 452, Loss: 0.3955440819263458\n",
      "Epoch 453, Loss: 0.3425431549549103\n",
      "Epoch 454, Loss: 0.3552224934101105\n",
      "Epoch 455, Loss: 0.4134821593761444\n",
      "Epoch 456, Loss: 0.41191819310188293\n",
      "Epoch 457, Loss: 0.20093730092048645\n",
      "Epoch 458, Loss: 0.32669758796691895\n",
      "Epoch 459, Loss: 0.2125183343887329\n",
      "Epoch 460, Loss: 0.37460845708847046\n",
      "Epoch 461, Loss: 0.16293828189373016\n",
      "Epoch 462, Loss: 0.3991216719150543\n",
      "Epoch 463, Loss: 0.373477965593338\n",
      "Epoch 464, Loss: 0.47107067704200745\n",
      "Epoch 465, Loss: 0.3842552900314331\n",
      "Epoch 466, Loss: 0.4496663510799408\n",
      "Epoch 467, Loss: 0.5403546690940857\n",
      "Epoch 468, Loss: 0.32639920711517334\n",
      "Epoch 469, Loss: 0.3964574337005615\n",
      "Epoch 470, Loss: 0.3232771158218384\n",
      "Epoch 471, Loss: 0.2947927415370941\n",
      "Epoch 472, Loss: 0.36969923973083496\n",
      "Epoch 473, Loss: 0.2878834009170532\n",
      "Epoch 474, Loss: 0.21629683673381805\n",
      "Epoch 475, Loss: 0.524886965751648\n",
      "Epoch 476, Loss: 0.38997480273246765\n",
      "Epoch 477, Loss: 0.23453181982040405\n",
      "Epoch 478, Loss: 0.40806251764297485\n",
      "Epoch 479, Loss: 0.2818073034286499\n",
      "Epoch 480, Loss: 0.2190842628479004\n",
      "Epoch 481, Loss: 0.44721484184265137\n",
      "Epoch 482, Loss: 0.3484462797641754\n",
      "Epoch 483, Loss: 0.3955402076244354\n",
      "Epoch 484, Loss: 0.2508079409599304\n",
      "Epoch 485, Loss: 0.35597243905067444\n",
      "Epoch 486, Loss: 0.41260623931884766\n",
      "Epoch 487, Loss: 0.34220996499061584\n",
      "Epoch 488, Loss: 0.25117212533950806\n",
      "Epoch 489, Loss: 0.2791098356246948\n",
      "Epoch 490, Loss: 0.45183584094047546\n",
      "Epoch 491, Loss: 0.46093639731407166\n",
      "Epoch 492, Loss: 0.4632371664047241\n",
      "Epoch 493, Loss: 0.2703980803489685\n",
      "Epoch 494, Loss: 0.5275389552116394\n",
      "Epoch 495, Loss: 0.5216006636619568\n",
      "Epoch 496, Loss: 0.3363044261932373\n",
      "Epoch 497, Loss: 0.5645878911018372\n",
      "Epoch 498, Loss: 0.21438415348529816\n",
      "Epoch 499, Loss: 0.26629242300987244\n",
      "Epoch 500, Loss: 0.26375001668930054\n",
      "Epoch 501, Loss: 0.5433244705200195\n",
      "Epoch 502, Loss: 0.25777629017829895\n",
      "Epoch 503, Loss: 0.2773134708404541\n",
      "Epoch 504, Loss: 0.3146638572216034\n",
      "Epoch 505, Loss: 0.24404262006282806\n",
      "Epoch 506, Loss: 0.2934271991252899\n",
      "Epoch 507, Loss: 0.36840078234672546\n",
      "Epoch 508, Loss: 0.31903350353240967\n",
      "Epoch 509, Loss: 0.3468306064605713\n",
      "Epoch 510, Loss: 0.32033708691596985\n",
      "Epoch 511, Loss: 0.5487360954284668\n",
      "Epoch 512, Loss: 0.38804975152015686\n",
      "Epoch 513, Loss: 0.5828815698623657\n",
      "Epoch 514, Loss: 0.33171066641807556\n",
      "Epoch 515, Loss: 0.43295159935951233\n",
      "Epoch 516, Loss: 0.5299594402313232\n",
      "Epoch 517, Loss: 0.32485848665237427\n",
      "Epoch 518, Loss: 0.23550397157669067\n",
      "Epoch 519, Loss: 0.5420189499855042\n",
      "Epoch 520, Loss: 0.39090582728385925\n",
      "Epoch 521, Loss: 0.29962775111198425\n",
      "Epoch 522, Loss: 0.39540722966194153\n",
      "Epoch 523, Loss: 0.13241927325725555\n",
      "Epoch 524, Loss: 0.45647239685058594\n",
      "Epoch 525, Loss: 0.23236170411109924\n",
      "Epoch 526, Loss: 0.301570862531662\n",
      "Epoch 527, Loss: 0.443689227104187\n",
      "Epoch 528, Loss: 0.3218434751033783\n",
      "Epoch 529, Loss: 0.30784595012664795\n",
      "Epoch 530, Loss: 0.3669833540916443\n",
      "Epoch 531, Loss: 0.2968006730079651\n",
      "Epoch 532, Loss: 0.19968591630458832\n",
      "Epoch 533, Loss: 0.3421041965484619\n",
      "Epoch 534, Loss: 0.4684850871562958\n",
      "Epoch 535, Loss: 0.2915622889995575\n",
      "Epoch 536, Loss: 0.21600507199764252\n",
      "Epoch 537, Loss: 0.41534069180488586\n",
      "Epoch 538, Loss: 0.3924644887447357\n",
      "Epoch 539, Loss: 0.48036178946495056\n",
      "Epoch 540, Loss: 0.3505918085575104\n",
      "Epoch 541, Loss: 0.577913224697113\n",
      "Epoch 542, Loss: 0.4154854118824005\n",
      "Epoch 543, Loss: 0.5796734690666199\n",
      "Epoch 544, Loss: 0.24735669791698456\n",
      "Epoch 545, Loss: 0.18286065757274628\n",
      "Epoch 546, Loss: 0.3829154968261719\n",
      "Epoch 547, Loss: 0.3213717043399811\n",
      "Epoch 548, Loss: 0.418119877576828\n",
      "Epoch 549, Loss: 0.22642171382904053\n",
      "Epoch 550, Loss: 0.1965116709470749\n",
      "Epoch 551, Loss: 0.41081294417381287\n",
      "Epoch 552, Loss: 0.400834858417511\n",
      "Epoch 553, Loss: 0.3942883014678955\n",
      "Epoch 554, Loss: 0.192626953125\n",
      "Epoch 555, Loss: 0.23267345130443573\n",
      "Epoch 556, Loss: 0.33884772658348083\n",
      "Epoch 557, Loss: 0.5233373045921326\n",
      "Epoch 558, Loss: 0.22411511838436127\n",
      "Epoch 559, Loss: 0.282152384519577\n",
      "Epoch 560, Loss: 0.339011549949646\n",
      "Epoch 561, Loss: 0.29732435941696167\n",
      "Epoch 562, Loss: 0.4623643457889557\n",
      "Epoch 563, Loss: 0.3696596622467041\n",
      "Epoch 564, Loss: 0.488226056098938\n",
      "Epoch 565, Loss: 0.2907659411430359\n",
      "Epoch 566, Loss: 0.36171844601631165\n",
      "Epoch 567, Loss: 0.2114616483449936\n",
      "Epoch 568, Loss: 0.20816929638385773\n",
      "Epoch 569, Loss: 0.403591513633728\n",
      "Epoch 570, Loss: 0.3110717535018921\n",
      "Epoch 571, Loss: 0.3018621504306793\n",
      "Epoch 572, Loss: 0.34953761100769043\n",
      "Epoch 573, Loss: 0.3732912838459015\n",
      "Epoch 574, Loss: 0.39218923449516296\n",
      "Epoch 575, Loss: 0.5594066977500916\n",
      "Epoch 576, Loss: 0.32947954535484314\n",
      "Epoch 577, Loss: 0.3392987847328186\n",
      "Epoch 578, Loss: 0.21147535741329193\n",
      "Epoch 579, Loss: 0.45179295539855957\n",
      "Epoch 580, Loss: 0.3458278477191925\n",
      "Epoch 581, Loss: 0.4825015366077423\n",
      "Epoch 582, Loss: 0.3066994845867157\n",
      "Epoch 583, Loss: 0.37438127398490906\n",
      "Epoch 584, Loss: 0.3306075632572174\n",
      "Epoch 585, Loss: 0.3963838517665863\n",
      "Epoch 586, Loss: 0.2642671465873718\n",
      "Epoch 587, Loss: 0.48381179571151733\n",
      "Epoch 588, Loss: 0.2909042537212372\n",
      "Epoch 589, Loss: 0.2460780292749405\n",
      "Epoch 590, Loss: 0.19520986080169678\n",
      "Epoch 591, Loss: 0.4070306718349457\n",
      "Epoch 592, Loss: 0.2603972554206848\n",
      "Epoch 593, Loss: 0.39750874042510986\n",
      "Epoch 594, Loss: 0.3532726466655731\n",
      "Epoch 595, Loss: 0.3572361171245575\n",
      "Epoch 596, Loss: 0.3319835662841797\n",
      "Epoch 597, Loss: 0.3496832251548767\n",
      "Epoch 598, Loss: 0.21345718204975128\n",
      "Epoch 599, Loss: 0.2442188560962677\n",
      "Epoch 600, Loss: 0.48152175545692444\n",
      "Epoch 601, Loss: 0.25593116879463196\n",
      "Epoch 602, Loss: 0.2892259359359741\n",
      "Epoch 603, Loss: 0.6223812699317932\n",
      "Epoch 604, Loss: 0.2631446421146393\n",
      "Epoch 605, Loss: 0.3159658908843994\n",
      "Epoch 606, Loss: 0.29873740673065186\n",
      "Epoch 607, Loss: 0.3785190284252167\n",
      "Epoch 608, Loss: 0.145668163895607\n",
      "Epoch 609, Loss: 0.5006928443908691\n",
      "Epoch 610, Loss: 0.3635692298412323\n",
      "Epoch 611, Loss: 0.31844937801361084\n",
      "Epoch 612, Loss: 0.3626388907432556\n",
      "Epoch 613, Loss: 0.4563758373260498\n",
      "Epoch 614, Loss: 0.27832627296447754\n",
      "Epoch 615, Loss: 0.4263448417186737\n",
      "Epoch 616, Loss: 0.22042283415794373\n",
      "Epoch 617, Loss: 0.1468701958656311\n",
      "Epoch 618, Loss: 0.4182985723018646\n",
      "Epoch 619, Loss: 0.639511227607727\n",
      "Epoch 620, Loss: 0.18018433451652527\n",
      "Epoch 621, Loss: 0.25623664259910583\n",
      "Epoch 622, Loss: 0.23655790090560913\n",
      "Epoch 623, Loss: 0.17012478411197662\n",
      "Epoch 624, Loss: 0.3691408336162567\n",
      "Epoch 625, Loss: 0.3817172348499298\n",
      "Epoch 626, Loss: 0.2641635835170746\n",
      "Epoch 627, Loss: 0.4396848678588867\n",
      "Epoch 628, Loss: 0.3937530517578125\n",
      "Epoch 629, Loss: 0.40055522322654724\n",
      "Epoch 630, Loss: 0.2650909721851349\n",
      "Epoch 631, Loss: 0.25740545988082886\n",
      "Epoch 632, Loss: 0.24851951003074646\n",
      "Epoch 633, Loss: 0.4218117296695709\n",
      "Epoch 634, Loss: 0.297159880399704\n",
      "Epoch 635, Loss: 0.477268785238266\n",
      "Epoch 636, Loss: 0.2591747045516968\n",
      "Epoch 637, Loss: 0.30058324337005615\n",
      "Epoch 638, Loss: 0.24139901995658875\n",
      "Epoch 639, Loss: 0.3443481922149658\n",
      "Epoch 640, Loss: 0.3146516978740692\n",
      "Epoch 641, Loss: 0.33565258979797363\n",
      "Epoch 642, Loss: 0.43209049105644226\n",
      "Epoch 643, Loss: 0.29216042160987854\n",
      "Epoch 644, Loss: 0.4408629238605499\n",
      "Epoch 645, Loss: 0.28827956318855286\n",
      "Epoch 646, Loss: 0.2827795147895813\n",
      "Epoch 647, Loss: 0.2745753824710846\n",
      "Epoch 648, Loss: 0.32282620668411255\n",
      "Epoch 649, Loss: 0.2141263782978058\n",
      "Epoch 650, Loss: 0.46472427248954773\n",
      "Epoch 651, Loss: 0.5200173258781433\n",
      "Epoch 652, Loss: 0.3865624964237213\n",
      "Epoch 653, Loss: 0.364284873008728\n",
      "Epoch 654, Loss: 0.2702697515487671\n",
      "Epoch 655, Loss: 0.3639845550060272\n",
      "Epoch 656, Loss: 0.23196528851985931\n",
      "Epoch 657, Loss: 0.2837800681591034\n",
      "Epoch 658, Loss: 0.4568083584308624\n",
      "Epoch 659, Loss: 0.3343923091888428\n",
      "Epoch 660, Loss: 0.3634391129016876\n",
      "Epoch 661, Loss: 0.2207472175359726\n",
      "Epoch 662, Loss: 0.3351079821586609\n",
      "Epoch 663, Loss: 0.28985002636909485\n",
      "Epoch 664, Loss: 0.24728010594844818\n",
      "Epoch 665, Loss: 0.21906611323356628\n",
      "Epoch 666, Loss: 0.1619744598865509\n",
      "Epoch 667, Loss: 0.29043012857437134\n",
      "Epoch 668, Loss: 0.2918454706668854\n",
      "Epoch 669, Loss: 0.25870946049690247\n",
      "Epoch 670, Loss: 0.3238239288330078\n",
      "Epoch 671, Loss: 0.44609296321868896\n",
      "Epoch 672, Loss: 0.3344306945800781\n",
      "Epoch 673, Loss: 0.3513822555541992\n",
      "Epoch 674, Loss: 0.23896761238574982\n",
      "Epoch 675, Loss: 0.38074374198913574\n",
      "Epoch 676, Loss: 0.3204517364501953\n",
      "Epoch 677, Loss: 0.2324495017528534\n",
      "Epoch 678, Loss: 0.375266969203949\n",
      "Epoch 679, Loss: 0.37975242733955383\n",
      "Epoch 680, Loss: 0.24984748661518097\n",
      "Epoch 681, Loss: 0.1644759476184845\n",
      "Epoch 682, Loss: 0.36346474289894104\n",
      "Epoch 683, Loss: 0.2575486898422241\n",
      "Epoch 684, Loss: 0.43822231888771057\n",
      "Epoch 685, Loss: 0.2143010050058365\n",
      "Epoch 686, Loss: 0.30196964740753174\n",
      "Epoch 687, Loss: 0.4350118339061737\n",
      "Epoch 688, Loss: 0.3051843047142029\n",
      "Epoch 689, Loss: 0.38349345326423645\n",
      "Epoch 690, Loss: 0.20396049320697784\n",
      "Epoch 691, Loss: 0.39218103885650635\n",
      "Epoch 692, Loss: 0.5059139132499695\n",
      "Epoch 693, Loss: 0.4984261691570282\n",
      "Epoch 694, Loss: 0.45877665281295776\n",
      "Epoch 695, Loss: 0.5784679055213928\n",
      "Epoch 696, Loss: 0.341959685087204\n",
      "Epoch 697, Loss: 0.16994570195674896\n",
      "Epoch 698, Loss: 0.46461886167526245\n",
      "Epoch 699, Loss: 0.18199466168880463\n",
      "Epoch 700, Loss: 0.3675602376461029\n",
      "Epoch 701, Loss: 0.21703273057937622\n",
      "Epoch 702, Loss: 0.1295330971479416\n",
      "Epoch 703, Loss: 0.3793521225452423\n",
      "Epoch 704, Loss: 0.42682909965515137\n",
      "Epoch 705, Loss: 0.25424903631210327\n",
      "Epoch 706, Loss: 0.30499929189682007\n",
      "Epoch 707, Loss: 0.2841324508190155\n",
      "Epoch 708, Loss: 0.6249768137931824\n",
      "Epoch 709, Loss: 0.3496834933757782\n",
      "Epoch 710, Loss: 0.3798880875110626\n",
      "Epoch 711, Loss: 0.2642171084880829\n",
      "Epoch 712, Loss: 0.4586550295352936\n",
      "Epoch 713, Loss: 0.22092904150485992\n",
      "Epoch 714, Loss: 0.1916588842868805\n",
      "Epoch 715, Loss: 0.2378951460123062\n",
      "Epoch 716, Loss: 0.18147924542427063\n",
      "Epoch 717, Loss: 0.40118667483329773\n",
      "Epoch 718, Loss: 0.3146171569824219\n",
      "Epoch 719, Loss: 0.4943903386592865\n",
      "Epoch 720, Loss: 0.2460063248872757\n",
      "Epoch 721, Loss: 0.21240991353988647\n",
      "Epoch 722, Loss: 0.4674375355243683\n",
      "Epoch 723, Loss: 0.44655582308769226\n",
      "Epoch 724, Loss: 0.3258749544620514\n",
      "Epoch 725, Loss: 0.5600505471229553\n",
      "Epoch 726, Loss: 0.27891093492507935\n",
      "Epoch 727, Loss: 0.5248527526855469\n",
      "Epoch 728, Loss: 0.25650689005851746\n",
      "Epoch 729, Loss: 0.16555090248584747\n",
      "Epoch 730, Loss: 0.31633657217025757\n",
      "Epoch 731, Loss: 0.32025936245918274\n",
      "Epoch 732, Loss: 0.18025906383991241\n",
      "Epoch 733, Loss: 0.2668917775154114\n",
      "Epoch 734, Loss: 0.3340483009815216\n",
      "Epoch 735, Loss: 0.20967136323451996\n",
      "Epoch 736, Loss: 0.1563439518213272\n",
      "Epoch 737, Loss: 0.23341165482997894\n",
      "Epoch 738, Loss: 0.22005565464496613\n",
      "Epoch 739, Loss: 0.29057735204696655\n",
      "Epoch 740, Loss: 0.27389514446258545\n",
      "Epoch 741, Loss: 0.3351341485977173\n",
      "Epoch 742, Loss: 0.40624937415122986\n",
      "Epoch 743, Loss: 0.2129906564950943\n",
      "Epoch 744, Loss: 0.40568244457244873\n",
      "Epoch 745, Loss: 0.4719901382923126\n",
      "Epoch 746, Loss: 0.4463154971599579\n",
      "Epoch 747, Loss: 0.3698325753211975\n",
      "Epoch 748, Loss: 0.4129702150821686\n",
      "Epoch 749, Loss: 0.41314956545829773\n",
      "Epoch 750, Loss: 0.5702671408653259\n",
      "Epoch 751, Loss: 0.4863211214542389\n",
      "Epoch 752, Loss: 0.4231244921684265\n",
      "Epoch 753, Loss: 0.3626762926578522\n",
      "Epoch 754, Loss: 0.22108706831932068\n",
      "Epoch 755, Loss: 0.23527854681015015\n",
      "Epoch 756, Loss: 0.2995206117630005\n",
      "Epoch 757, Loss: 0.3651977777481079\n",
      "Epoch 758, Loss: 0.3099061846733093\n",
      "Epoch 759, Loss: 0.4362621307373047\n",
      "Epoch 760, Loss: 0.1771591305732727\n",
      "Epoch 761, Loss: 0.4384605586528778\n",
      "Epoch 762, Loss: 0.3364854156970978\n",
      "Epoch 763, Loss: 0.47552964091300964\n",
      "Epoch 764, Loss: 0.2934837341308594\n",
      "Epoch 765, Loss: 0.334024578332901\n",
      "Epoch 766, Loss: 0.1537868231534958\n",
      "Epoch 767, Loss: 0.37897855043411255\n",
      "Epoch 768, Loss: 0.2099343240261078\n",
      "Epoch 769, Loss: 0.2669808864593506\n",
      "Epoch 770, Loss: 0.4902012348175049\n",
      "Epoch 771, Loss: 0.47193795442581177\n",
      "Epoch 772, Loss: 0.19263975322246552\n",
      "Epoch 773, Loss: 0.29662588238716125\n",
      "Epoch 774, Loss: 0.1744723618030548\n",
      "Epoch 775, Loss: 0.4752872586250305\n",
      "Epoch 776, Loss: 0.3989269733428955\n",
      "Epoch 777, Loss: 0.5224530100822449\n",
      "Epoch 778, Loss: 0.4743627607822418\n",
      "Epoch 779, Loss: 0.31039002537727356\n",
      "Epoch 780, Loss: 0.3266562819480896\n",
      "Epoch 781, Loss: 0.5488888025283813\n",
      "Epoch 782, Loss: 0.25484126806259155\n",
      "Epoch 783, Loss: 0.5106425285339355\n",
      "Epoch 784, Loss: 0.26232606172561646\n",
      "Epoch 785, Loss: 0.24868027865886688\n",
      "Epoch 786, Loss: 0.32255229353904724\n",
      "Epoch 787, Loss: 0.32482123374938965\n",
      "Epoch 788, Loss: 0.31553614139556885\n",
      "Epoch 789, Loss: 0.39105677604675293\n",
      "Epoch 790, Loss: 0.23168836534023285\n",
      "Epoch 791, Loss: 0.41717731952667236\n",
      "Epoch 792, Loss: 0.4640674591064453\n",
      "Epoch 793, Loss: 0.38239315152168274\n",
      "Epoch 794, Loss: 0.2258782982826233\n",
      "Epoch 795, Loss: 0.4100908935070038\n",
      "Epoch 796, Loss: 0.4087342917919159\n",
      "Epoch 797, Loss: 0.34629446268081665\n",
      "Epoch 798, Loss: 0.14019569754600525\n",
      "Epoch 799, Loss: 0.31089797616004944\n",
      "Epoch 800, Loss: 0.27500709891319275\n",
      "Epoch 801, Loss: 0.26245805621147156\n",
      "Epoch 802, Loss: 0.5060622692108154\n",
      "Epoch 803, Loss: 0.37553083896636963\n",
      "Epoch 804, Loss: 0.35855427384376526\n",
      "Epoch 805, Loss: 0.2664949297904968\n",
      "Epoch 806, Loss: 0.22493383288383484\n",
      "Epoch 807, Loss: 0.2666761875152588\n",
      "Epoch 808, Loss: 0.45174869894981384\n",
      "Epoch 809, Loss: 0.34076958894729614\n",
      "Epoch 810, Loss: 0.3289218544960022\n",
      "Epoch 811, Loss: 0.5115390419960022\n",
      "Epoch 812, Loss: 0.2651130259037018\n",
      "Epoch 813, Loss: 0.18804015219211578\n",
      "Epoch 814, Loss: 0.3525318503379822\n",
      "Epoch 815, Loss: 0.3845962584018707\n",
      "Epoch 816, Loss: 0.4043777585029602\n",
      "Epoch 817, Loss: 0.35939276218414307\n",
      "Epoch 818, Loss: 0.25605228543281555\n",
      "Epoch 819, Loss: 0.19383306801319122\n",
      "Epoch 820, Loss: 0.3844831883907318\n",
      "Epoch 821, Loss: 0.4748668670654297\n",
      "Epoch 822, Loss: 0.6057757139205933\n",
      "Epoch 823, Loss: 0.22046445310115814\n",
      "Epoch 824, Loss: 0.4806405305862427\n",
      "Epoch 825, Loss: 0.3871675133705139\n",
      "Epoch 826, Loss: 0.22041301429271698\n",
      "Epoch 827, Loss: 0.3172212839126587\n",
      "Epoch 828, Loss: 0.18929949402809143\n",
      "Epoch 829, Loss: 0.25907397270202637\n",
      "Epoch 830, Loss: 0.6385924220085144\n",
      "Epoch 831, Loss: 0.26022809743881226\n",
      "Epoch 832, Loss: 0.3463575541973114\n",
      "Epoch 833, Loss: 0.25643736124038696\n",
      "Epoch 834, Loss: 0.20148524641990662\n",
      "Epoch 835, Loss: 0.42794057726860046\n",
      "Epoch 836, Loss: 0.5335657596588135\n",
      "Epoch 837, Loss: 0.3981587588787079\n",
      "Epoch 838, Loss: 0.2094925194978714\n",
      "Epoch 839, Loss: 0.29551395773887634\n",
      "Epoch 840, Loss: 0.2693810760974884\n",
      "Epoch 841, Loss: 0.310077041387558\n",
      "Epoch 842, Loss: 0.39439836144447327\n",
      "Epoch 843, Loss: 0.3864772915840149\n",
      "Epoch 844, Loss: 0.34909898042678833\n",
      "Epoch 845, Loss: 0.3322692811489105\n",
      "Epoch 846, Loss: 0.34553074836730957\n",
      "Epoch 847, Loss: 0.36110368371009827\n",
      "Epoch 848, Loss: 0.17622394859790802\n",
      "Epoch 849, Loss: 0.42329248785972595\n",
      "Epoch 850, Loss: 0.2982480525970459\n",
      "Epoch 851, Loss: 0.3903110921382904\n",
      "Epoch 852, Loss: 0.49925360083580017\n",
      "Epoch 853, Loss: 0.26162856817245483\n",
      "Epoch 854, Loss: 0.35468795895576477\n",
      "Epoch 855, Loss: 0.41300857067108154\n",
      "Epoch 856, Loss: 0.3423728942871094\n",
      "Epoch 857, Loss: 0.2314397692680359\n",
      "Epoch 858, Loss: 0.34081751108169556\n",
      "Epoch 859, Loss: 0.4647829830646515\n",
      "Epoch 860, Loss: 0.3304581046104431\n",
      "Epoch 861, Loss: 0.35398179292678833\n",
      "Epoch 862, Loss: 0.14237695932388306\n",
      "Epoch 863, Loss: 0.3260655105113983\n",
      "Epoch 864, Loss: 0.524868905544281\n",
      "Epoch 865, Loss: 0.4633220136165619\n",
      "Epoch 866, Loss: 0.3792184293270111\n",
      "Epoch 867, Loss: 0.11745649576187134\n",
      "Epoch 868, Loss: 0.3572690784931183\n",
      "Epoch 869, Loss: 0.25181490182876587\n",
      "Epoch 870, Loss: 0.22063995897769928\n",
      "Epoch 871, Loss: 0.40635478496551514\n",
      "Epoch 872, Loss: 0.22399133443832397\n",
      "Epoch 873, Loss: 0.3767419755458832\n",
      "Epoch 874, Loss: 0.4532652497291565\n",
      "Epoch 875, Loss: 0.3484894037246704\n",
      "Epoch 876, Loss: 0.4535626173019409\n",
      "Epoch 877, Loss: 0.38519057631492615\n",
      "Epoch 878, Loss: 0.32835033535957336\n",
      "Epoch 879, Loss: 0.40978941321372986\n",
      "Epoch 880, Loss: 0.27489742636680603\n",
      "Epoch 881, Loss: 0.2894090712070465\n",
      "Epoch 882, Loss: 0.2211877703666687\n",
      "Epoch 883, Loss: 0.32501474022865295\n",
      "Epoch 884, Loss: 0.27797767519950867\n",
      "Epoch 885, Loss: 0.4088740944862366\n",
      "Epoch 886, Loss: 0.4188821017742157\n",
      "Epoch 887, Loss: 0.3767315447330475\n",
      "Epoch 888, Loss: 0.5526975393295288\n",
      "Epoch 889, Loss: 0.3529382348060608\n",
      "Epoch 890, Loss: 0.29417920112609863\n",
      "Epoch 891, Loss: 0.20641709864139557\n",
      "Epoch 892, Loss: 0.48523813486099243\n",
      "Epoch 893, Loss: 0.41425690054893494\n",
      "Epoch 894, Loss: 0.35598278045654297\n",
      "Epoch 895, Loss: 0.21668502688407898\n",
      "Epoch 896, Loss: 0.3023226261138916\n",
      "Epoch 897, Loss: 0.25244563817977905\n",
      "Epoch 898, Loss: 0.41920211911201477\n",
      "Epoch 899, Loss: 0.39886513352394104\n",
      "Epoch 900, Loss: 0.35305431485176086\n",
      "Epoch 901, Loss: 0.1804475635290146\n",
      "Epoch 902, Loss: 0.2967763841152191\n",
      "Epoch 903, Loss: 0.26783618330955505\n",
      "Epoch 904, Loss: 0.20049072802066803\n",
      "Epoch 905, Loss: 0.15042400360107422\n",
      "Epoch 906, Loss: 0.18918800354003906\n",
      "Epoch 907, Loss: 0.39868471026420593\n",
      "Epoch 908, Loss: 0.3988097906112671\n",
      "Epoch 909, Loss: 0.36794573068618774\n",
      "Epoch 910, Loss: 0.3001599907875061\n",
      "Epoch 911, Loss: 0.42530256509780884\n",
      "Epoch 912, Loss: 0.325459748506546\n",
      "Epoch 913, Loss: 0.38876956701278687\n",
      "Epoch 914, Loss: 0.14438144862651825\n",
      "Epoch 915, Loss: 0.2291838377714157\n",
      "Epoch 916, Loss: 0.3173695206642151\n",
      "Epoch 917, Loss: 0.3888917863368988\n",
      "Epoch 918, Loss: 0.2341691106557846\n",
      "Epoch 919, Loss: 0.2023262083530426\n",
      "Epoch 920, Loss: 0.37234851717948914\n",
      "Epoch 921, Loss: 0.43088436126708984\n",
      "Epoch 922, Loss: 0.342831552028656\n",
      "Epoch 923, Loss: 0.36463889479637146\n",
      "Epoch 924, Loss: 0.39652055501937866\n",
      "Epoch 925, Loss: 0.39852964878082275\n",
      "Epoch 926, Loss: 0.41550299525260925\n",
      "Epoch 927, Loss: 0.47666627168655396\n",
      "Epoch 928, Loss: 0.2674624025821686\n",
      "Epoch 929, Loss: 0.7867339253425598\n",
      "Epoch 930, Loss: 0.3667035698890686\n",
      "Epoch 931, Loss: 0.3361208140850067\n",
      "Epoch 932, Loss: 0.28497177362442017\n",
      "Epoch 933, Loss: 0.3502897024154663\n",
      "Epoch 934, Loss: 0.44931760430336\n",
      "Epoch 935, Loss: 0.39288660883903503\n",
      "Epoch 936, Loss: 0.18185441195964813\n",
      "Epoch 937, Loss: 0.32392632961273193\n",
      "Epoch 938, Loss: 0.39135223627090454\n",
      "Epoch 939, Loss: 0.23055657744407654\n",
      "Epoch 940, Loss: 0.35696470737457275\n",
      "Epoch 941, Loss: 0.35803526639938354\n",
      "Epoch 942, Loss: 0.205028235912323\n",
      "Epoch 943, Loss: 0.3100637197494507\n",
      "Epoch 944, Loss: 0.15372882783412933\n",
      "Epoch 945, Loss: 0.19464008510112762\n",
      "Epoch 946, Loss: 0.31254297494888306\n",
      "Epoch 947, Loss: 0.275960773229599\n",
      "Epoch 948, Loss: 0.4966491460800171\n",
      "Epoch 949, Loss: 0.49894440174102783\n",
      "Epoch 950, Loss: 0.19630347192287445\n",
      "Epoch 951, Loss: 0.7799437046051025\n",
      "Epoch 952, Loss: 0.2232595831155777\n",
      "Epoch 953, Loss: 0.4834636449813843\n",
      "Epoch 954, Loss: 0.41009214520454407\n",
      "Epoch 955, Loss: 0.3767261207103729\n",
      "Epoch 956, Loss: 0.34406954050064087\n",
      "Epoch 957, Loss: 0.262077659368515\n",
      "Epoch 958, Loss: 0.452338844537735\n",
      "Epoch 959, Loss: 0.4837646782398224\n",
      "Epoch 960, Loss: 0.44042259454727173\n",
      "Epoch 961, Loss: 0.32056543231010437\n",
      "Epoch 962, Loss: 0.15756718814373016\n",
      "Epoch 963, Loss: 0.32637107372283936\n",
      "Epoch 964, Loss: 0.5556328296661377\n",
      "Epoch 965, Loss: 0.18320897221565247\n",
      "Epoch 966, Loss: 0.3085609972476959\n",
      "Epoch 967, Loss: 0.41811344027519226\n",
      "Epoch 968, Loss: 0.38152632117271423\n",
      "Epoch 969, Loss: 0.16432145237922668\n",
      "Epoch 970, Loss: 0.2622308135032654\n",
      "Epoch 971, Loss: 0.31048521399497986\n",
      "Epoch 972, Loss: 0.16966909170150757\n",
      "Epoch 973, Loss: 0.4237702786922455\n",
      "Epoch 974, Loss: 0.2977401912212372\n",
      "Epoch 975, Loss: 0.18074344098567963\n",
      "Epoch 976, Loss: 0.22356750071048737\n",
      "Epoch 977, Loss: 0.594479501247406\n",
      "Epoch 978, Loss: 0.44931143522262573\n",
      "Epoch 979, Loss: 0.19112814962863922\n",
      "Epoch 980, Loss: 0.34164178371429443\n",
      "Epoch 981, Loss: 0.3785445988178253\n",
      "Epoch 982, Loss: 0.4149915874004364\n",
      "Epoch 983, Loss: 0.502267599105835\n",
      "Epoch 984, Loss: 0.24510879814624786\n",
      "Epoch 985, Loss: 0.35282090306282043\n",
      "Epoch 986, Loss: 0.33313870429992676\n",
      "Epoch 987, Loss: 0.3164737820625305\n",
      "Epoch 988, Loss: 0.25395667552948\n",
      "Epoch 989, Loss: 0.389721542596817\n",
      "Epoch 990, Loss: 0.2619251310825348\n",
      "Epoch 991, Loss: 0.37524664402008057\n",
      "Epoch 992, Loss: 0.29429978132247925\n",
      "Epoch 993, Loss: 0.26468366384506226\n",
      "Epoch 994, Loss: 0.37008824944496155\n",
      "Epoch 995, Loss: 0.25584864616394043\n",
      "Epoch 996, Loss: 0.26319420337677\n",
      "Epoch 997, Loss: 0.3610492944717407\n",
      "Epoch 998, Loss: 0.23904956877231598\n",
      "Epoch 999, Loss: 0.17960458993911743\n",
      "Epoch 1000, Loss: 0.37206122279167175\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Check if y_train and y_test are Pandas Series or DataFrame and convert to Numpy array if necessary\n",
    "if isinstance(y_train, (pd.Series, pd.DataFrame)):\n",
    "    y_train = y_train.to_numpy()  # or y_train.values for older Pandas versions\n",
    "\n",
    "# Transform data to tensor\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)  # Directly convert to LongTensor\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_data_tensor = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data_tensor, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Build NN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# Train NN model\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = Variable(X_batch), Variable(y_batch)\n",
    "        net.zero_grad()\n",
    "        output = net(X_batch)\n",
    "        loss = loss_func(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nn model to predict\n",
    "test_data = TensorDataset(X_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "y_pred_nn = []\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        output = net(X_batch[0])\n",
    "        for idx, i in enumerate(output):\n",
    "            y_pred_nn.append(torch.argmax(i))\n",
    "y_pred_nn = np.array(y_pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "result = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': y_pred_nn})\n",
    "result.to_csv('result_nn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
